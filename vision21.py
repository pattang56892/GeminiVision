import os
import json
import re
import base64
import textwrap
from datetime import datetime
import google.generativeai as genai
from dotenv import load_dotenv
from PIL import Image


# ‚úÖ Handle timezone correctly
try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except ImportError:
    from pytz import timezone  # For older Python versions

# ‚úÖ Load environment variables
load_dotenv()

# ‚úÖ Configure Gemini model
genai.configure(api_key=os.getenv("GOOGLE_GEMINI_1_5_API_KEY"))
model = genai.GenerativeModel("gemini-2.0-flash-exp")  

# ‚úÖ Ensure Eastern Time (New York / Toronto)
EASTERN_TIME = ZoneInfo("America/New_York") if "ZoneInfo" in globals() else timezone("America/New_York")

# ‚úÖ Dynamically determine JSON filename
script_name = os.path.splitext(os.path.basename(__file__))[0]
data_filename = f"{script_name}_data.json"

# ‚úÖ Generate a session ID for tracking
session_id = f"session_{datetime.now(EASTERN_TIME).strftime('%Y%m%d_%H%M%S')}"

# ‚úÖ Paths to local images
image_paths = ["uploads/p41.png", "uploads/p42.png"]

# ‚úÖ Function to load images as Base64
def load_image_as_base64(image_path):
    """Reads an image file and converts it to Base64 format."""
    if not os.path.exists(image_path):
        print(f"‚ö†Ô∏è Warning: The file '{image_path}' was not found. Skipping it.")
        return None

    try:
        with open(image_path, "rb") as img_file:
            base64_str = base64.b64encode(img_file.read()).decode("utf-8")
            mime_type = "image/png"

            if image_path.lower().endswith((".jpg", ".jpeg")):
                mime_type = "image/jpeg"
            elif image_path.lower().endswith(".gif"):
                mime_type = "image/gif"
            elif image_path.lower().endswith(".webp"):
                mime_type = "image/webp"

            return {"mime_type": mime_type, "data": base64_str}

    except Exception as e:
        print(f"‚ùå Error loading image '{image_path}': {e}")
        return None

# ‚úÖ Attempt to load images
images = [img for path in image_paths if (img := load_image_as_base64(path))]

# ‚úÖ Generate response from image & text with error handling
def generate_response(prompt):
    contents = images + [prompt]

    try:
        response = model.generate_content(contents)
        return response.text
    except Exception as e:
        print(f"‚ùå Error processing your request: {e}")
        return None  # Prevent saving error messages as valid responses

# ‚úÖ Load existing conversation history
def load_existing_conversation(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
                return existing_data.get("messages", [])
        except json.JSONDecodeError:
            return []
    return []

# ‚úÖ Append messages incrementally (instead of overwriting)
def save_messages_incrementally(file_path, user_message, assistant_message):
    messages = load_existing_conversation(file_path)

    # ‚úÖ Add only valid messages
    if assistant_message["content"]:  
        messages.append(user_message)
        messages.append(assistant_message)

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump({"messages": messages}, f, ensure_ascii=False, indent=4)
        except IOError as e:
            print(f"‚ùå Error writing to JSON file: {e}")

# ‚úÖ Convert conversation data into readable text format
def format_conversation_to_text(input_path, output_path, max_width=80):  # Added max_width
    try:
        with open(input_path, 'r', encoding="utf-8") as f:
            data = json.load(f)
            messages = data.get('messages', [])

        output_lines = []
        for msg in messages:
            timestamp = msg.get('timestamp', 'Unknown Time')
            role = msg.get('role', 'Unknown')
            content = msg.get('content', 'No Content')

            if role == 'user':
                role_label = 'User'
            elif role == 'assistant':
                role_label = 'Assistant'
            else:
                role_label = f"Unknown role {role}"

            # Wrap the content using textwrap
            wrapped_content = textwrap.fill(content, width=max_width)
            output_lines.append(f"[{timestamp}] {role_label}: {wrapped_content}")
            output_lines.append("")

        with open(output_path, 'w', encoding="utf-8") as f:
            f.write('\n'.join(output_lines))

        print(f"üìÑ Formatted conversation saved to: {output_path}")

    except Exception as e:
        print(f"‚ùå Error formatting conversation: {e}")

# ‚úÖ Main Chat Function
def chat_with_model():
    print("üîπ Welcome to the Gemini Vision Assistant!")
    print(f"üíæ Storing conversation in: {data_filename}")

    messages = load_existing_conversation(data_filename)

    while True:
        prompt = input("Your prompt: ").strip()
        if prompt.lower() == "exit":
            print("üëã Exiting chat. Goodbye!")
            break
        if not prompt:
            continue  # Skip empty input

        # ‚úÖ Create timestamped user message
        timestamp = datetime.now(EASTERN_TIME).isoformat()
        user_message = {
            "session_id": session_id,
            "timestamp": timestamp,
            "role": "user",
            "content": prompt
        }
        messages.append(user_message)

        # ‚úÖ Generate response from Gemini
        response_text = generate_response(prompt)

        if response_text is None:
            print("‚ö†Ô∏è No valid response received. Skipping message storage.")
            continue  # Skip saving the response if it's an error

        # ‚úÖ Create timestamped assistant response
        assistant_message = {
            "session_id": session_id,
            "timestamp": datetime.now(EASTERN_TIME).isoformat(),
            "role": "assistant",
            "content": response_text
        }
        messages.append(assistant_message)

        # ‚úÖ Save conversation incrementally
        save_messages_incrementally(data_filename, user_message, assistant_message)

        # ‚úÖ Save formatted output
        format_conversation_to_text(data_filename, f"{script_name}_formatted_data.txt")

        print("\nü§ñ **Gemini's response:**\n")
        print(response_text)
        print("‚Äî" * 40)

# ‚úÖ Run the chat session
if __name__ == "__main__":
    chat_with_model()
